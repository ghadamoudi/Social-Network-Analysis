{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)\n",
    "In this notebook we will introduce the basic concepts of NLP. NLP is the traditional term for intelligent text processing where a computer program tries to interpret what is written in natural language text or speech using computational linguistic methods. “ [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will do some basic NLP.\n",
    "- Part 1: Basic NLP Tasks\n",
    "    1. Segmentation and tekenizatiom\n",
    "    2. Stemming and lemmatization\n",
    "    3. Stopword\n",
    "    4. Word representation\n",
    "        - Bag-of-words\n",
    "        - TF.IDF\n",
    "<br> <br>  \n",
    "- Part 2: Creating an NLP Pipeline for Classification\n",
    "    1. Importing the data\n",
    "    2. Cleaning and preprocessing \n",
    "    3. Word representation\n",
    "    4. Applying the classification models\n",
    "    5. Using a classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pandas** is a python package that deals mostly with :\n",
    "- **Series**  (1d homogeneous array)\n",
    "- **DataFrame** (2d labeled heterogeneous array) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**matplotlib.pyplot** is a collection of functions that where each function makes some change to a figure: e.g., creates a figure, creates a plotting area in a figure, plots some lines in a plotting area, decorates the plot with labels, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLTK**: The Natural Language Toolkit, a platform used for building Python programs that work with human language data. \n",
    "- Contains text processing libraries for tokenization, parsing, classification, stemming, tagging and semantic reasoning. \n",
    "- Includes graphical demonstrations and sample data sets \n",
    "- Includes a cook book and a book which explains the principles behind the underlying language processing tasks that NLTK supports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do some basic text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My String'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"My String\"\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(s.index(\"y\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my string\n"
     ]
    }
   ],
   "source": [
    "print(s.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'String']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "afewwords = s.split(\" \")\n",
    "afewwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String starts with 'My'. Good!\n",
      "String ends with 'ing'. Good!\n"
     ]
    }
   ],
   "source": [
    "# Check how a string starts\n",
    "if s.startswith(\"My\"):\n",
    "    print(\"String starts with 'My'. Good!\")\n",
    "else:\n",
    "    print(\"Naah!\")\n",
    "\n",
    "# Check how a string ends\n",
    "if s.endswith(\"ing\"):\n",
    "    print(\"String ends with 'ing'. Good!\")\n",
    "else:\n",
    "    print(\"Naah!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic NLP Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Segmentation and Tokenisation\n",
    "\n",
    "- Separating sentences/ words from each other. \n",
    "- Must think about issues such as delimiters.. \n",
    "- Human written text such as tweets or clinical text is noisy, contains abbreviations, tokenizing can be difficult.   \n",
    "\n",
    "Chunking whitespace seems very easy, but it's not, because the text contains punctuation and contractions. Let's start with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'coolest', 'job', 'in', 'the', 'next', '10', 'years', 'will', 'be', 'statisticians.', 'People', 'think', \"I'm\", 'joking,', 'but', 'who', \"would've\", 'guessed', 'that', 'computer', 'engineers', \"would've\", 'been', 'the', 'coolest', 'job', 'of', 'the', '1990s?']\n"
     ]
    }
   ],
   "source": [
    "my_text = \"The coolest job in the next 10 years will be \" +\\\n",
    "              \"statisticians. People think I'm joking, but \" +\\\n",
    "              \"who would've guessed that computer engineers \" +\\\n",
    "              \"would've been the coolest job of the 1990s?\"\n",
    "simple_tokens = my_text.split(' ')\n",
    "print (simple_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No perfect! Some words didn't split like would've. let's try NLTK... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'coolest', 'job', 'in', 'the', 'next', '10', 'years', 'will', 'be', 'statisticians', '.', 'People', 'think', 'I', \"'m\", 'joking', ',', 'but', 'who', 'would', \"'ve\", 'guessed', 'that', 'computer', 'engineers', 'would', \"'ve\", 'been', 'the', 'coolest', 'job', 'of', 'the', '1990s', '?']\n"
     ]
    }
   ],
   "source": [
    "nltk_tokens = nltk.word_tokenize(my_text)\n",
    "print (nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Stemming and Lemmatization\n",
    "\n",
    "- Both aims at finding the root of the word.\n",
    "- Stemming: may not be a word and may not have an entry in the dictionary. \n",
    "- Example: beautiful --> beauti\n",
    "- Lemmatization: the root must have an entry in the dictionary.\n",
    "  \n",
    "**Python Implementation** \n",
    "There are a number of different stemmers in Python, these are:\n",
    "- Porter stemmer\n",
    "- Lancaster stemmer\n",
    "- Regex based stemmer\n",
    "- Snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'coolest', 'job', 'in', 'the', 'next', '10', 'year', 'wil', 'be', 'stat', '.', 'peopl', 'think', 'i', \"'m\", 'jok', ',', 'but', 'who', 'would', \"'ve\", 'guess', 'that', 'comput', 'engin', 'would', \"'ve\", 'been', 'the', 'coolest', 'job', 'of', 'the', '1990s', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import *\n",
    "stemmer = LancasterStemmer()\n",
    "print ([stemmer.stem(word) for word in nltk_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how statistician --> root, stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stopwords\n",
    "- Stopwords are the least informative (tokens) in text.\n",
    "- Most common words (such as the, it, is, as, and not). \n",
    "- Stopwords are often removed. \n",
    "- When removed, the processing takes less time and less memory; and sometimes more accurate. \n",
    "- They are high in frequency, approximately 40% of the words in a text. \n",
    "- There are around 200 typical stop words in each language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Word representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bag of words\n",
    "- TF IDF\n",
    "- Other advanced methods, word embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>44</th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   44  cab  call  me  please  tonight  you\n",
       "0   0    0     1   0       0        1    1\n",
       "1   0    1     1   1       0        0    0\n",
       "2   1    0     1   1       2        0    0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple count vectorizer example\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# start with a simple example\n",
    "train_simple = ['call you tonight','Call me a cab','please call me... PLEASE 44!']\n",
    "\n",
    "# learn the 'vocabulary' of the training data\n",
    "vect = CountVectorizer()\n",
    "\n",
    "train_simple_dtm = vect.fit_transform(train_simple)\n",
    "\n",
    "pd.DataFrame(train_simple_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>44</th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   44  cab  call  me  please  tonight  you\n",
       "0   0    0     1   1       1        0    0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data into a document-term matrix (using existing vocabulary, notice don't is missing)\n",
    "\n",
    "test_simple = [\"please don't call me\"]\n",
    "test_simple_dtm = vect.transform(test_simple)\n",
    "test_simple_dtm.toarray()\n",
    "pd.DataFrame(test_simple_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF IDF: Term Frequency / Inverse Document Frequency\n",
    "\n",
    "- Each word has a TF and IDF score\n",
    "- Term frequency: **how many times the term appears in a document.** \n",
    "- Inverse Document Frequency: **reflects how important a word is to a document in a collection.** \n",
    "\n",
    "**Example** <br>\n",
    "**TF:** a document containing 100 words and the word math apears 5 times, then TF for the word math is 5/100 = 0.05. <br>\n",
    "**IDF:** if the word math appears in 10 documents and we have total of 50 document then IDF = log(50/10) = 1.6\n",
    "   \n",
    "- TF.IDF for the word math in this context is: 0.05 X 1.6 = 0.08\n",
    "- The higher the TF.IDF the rarer the term and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Creating an NLP Pipeline for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing a sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                                msg\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_table('https://raw.githubusercontent.com/sinanuozdemir/sfdat22/master/data/sms.tsv', sep='\\t', header=None, names=['label', 'msg'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd15e9a3c50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.label.value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the probability that an email is a spam or ham: divide the number of ham/total eamils: (df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.865937\n",
       "spam    0.134063\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()/df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cleaning and preprocessing\n",
    "As we saw above tokenization, stemming, and stopword removal can all be part of the cleaning and preprocessing stage. in addition to the below:\n",
    "- Removing HTML tags \n",
    "- Removing special characters\n",
    "- Expanding contractions, he's --> he is\n",
    "- Case conversion \n",
    "- Spelling correction <br> <br>\n",
    "And more/less depending on the task.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create a function that gourp some cleaning taks in one command.\n",
    "\n",
    "def clean_and_stem_text(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    clean_tokens = [word for word in tokens if word not in stop_words]\n",
    "    stem_tokens = [stemmer.stem(token) for token in clean_tokens]\n",
    "    return \" \".join(stem_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coolest job next 10 year stat . peopl think 'm jok , would 've guess comput engin would 've coolest job 1990s ?\n"
     ]
    }
   ],
   "source": [
    "# testing the function with sample text\n",
    "\n",
    "my_text = \"The coolest job in the next 10 years will be \" +\\\n",
    "              \"statisticians. People think I'm joking, but \" +\\\n",
    "              \"who would've guessed that computer engineers \" +\\\n",
    "              \"would've been the coolest job of the 1990s?\"\n",
    "\n",
    "print(clean_and_stem_text(my_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['msg'] = df['msg'].apply(clean_and_stem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go jurong point , crazy.. avail bug n gre worl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar ... jok wif u on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>fre entry 2 wkly comp win fa cup fin tkts 21st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say ear hor ... u c already say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah n't think goe usf , liv around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                msg\n",
       "0   ham  go jurong point , crazy.. avail bug n gre worl...\n",
       "1   ham                        ok lar ... jok wif u on ...\n",
       "2  spam  fre entry 2 wkly comp win fa cup fin tkts 21st...\n",
       "3   ham          u dun say ear hor ... u c already say ...\n",
       "4   ham          nah n't think goe usf , liv around though"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Word representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use TF IDF instead of bag of words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first thing first...\n",
    "We have to split our data to training and testing, X denote features (the words) and Y denote target (label = spam / ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['msg'], df['label'], test_size = 0.3, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Applying the classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1000, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svm = SVC(C=1000)\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1442    0]\n",
      " [  30  200]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "X_test = vectorizer.transform(X_test)\n",
    "y_pred = svm.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True negative : actually ham predicted ham   = 489 <br>\n",
    "True positive : actually spam predicted spam = 64 <br>\n",
    "False negative: actually ham predicted spam  = 5 <br>\n",
    "False positive: actually spam predicted ham  = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9820574162679426"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = svm.predict(X_test)\n",
    "accuracy_score(y_test , pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila!! Very high accuracy, now you have a calssification model that you can use..\n",
    "We will try another classifier and compare, then we can select the best performing model and use it to classify new emails messages.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1441    1]\n",
      " [  25  205]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB(alpha=0.2)\n",
    "mnb.fit(X_train, y_train)\n",
    "y_pred = mnb.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9844497607655502"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = mnb.predict(X_test)\n",
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which it the best one? seems like NB is better.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Using the classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we used NB, (mnb)\n",
    "\n",
    "def pred(msg):    \n",
    "    msg = vectorizer.transform([msg])    \n",
    "    prediction = mnb.predict(msg)    \n",
    "    return prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred(\"hello dear you invited to my party\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred(\"WINNER$$$$ SMS Reply 'WIN'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Clinical Text Mining Secondary Use of Patient Health Records, Hercules Dalianis,  \n",
    "Springer Open, eBook https://doi.org/10.1007/978- 3- 319- 78503- 5\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
