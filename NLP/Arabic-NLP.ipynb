{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset was obtained from Heedzy at: https://heedzy.com/. It includes 100 reviews of Hungerstation App."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV data file\n",
    "\n",
    "df = pd.read_csv(\"hungerstation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "id": "hm6pa5Af4qbE",
    "outputId": "7bf41859-f291-44ae-a785-12cd486a53a1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "      <th>Name</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HungerStation</td>\n",
       "      <td>2021-11-02</td>\n",
       "      <td>Ø§ØªÙ…Ù†Ù‰ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡</td>\n",
       "      <td>ØªØºÙ„ÙŠÙ Ø§Ù„Ù…Ø´Ø±ÙˆØ¨Ø§Øª Ø¨Ø§Ø­ÙƒØ§Ù… (Ø§Ù„Ø­Ø§Ø± ÙˆØ§Ù„Ø¨Ø§Ø±Ø¯) Ù„Ù„ØªØ§ÙƒØ¯ ...</td>\n",
       "      <td>'Ø£Ù…ÙŠØ±Ø©'</td>\n",
       "      <td>2</td>\n",
       "      <td>5.13.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HungerStation</td>\n",
       "      <td>2021-11-02</td>\n",
       "      <td>Ø§Ø¹Ù„Ø§Ù†Ø§ØªÙ‡Ù… Ø£ÙƒØ¨Ø± Ù…Ù† ÙˆØ§Ù‚Ø¹Ù‡Ù…</td>\n",
       "      <td>Ø¬Ø§Ù‡Ø² ÙˆØ¨Ø³</td>\n",
       "      <td>Ø¯Ø­ 911</td>\n",
       "      <td>1</td>\n",
       "      <td>5.13.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HungerStation</td>\n",
       "      <td>2021-11-02</td>\n",
       "      <td>Ø³ÙŠØ¡ Ø¬Ø¯Ø§</td>\n",
       "      <td>Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø§ÙŠ Ø§Ù„ØªØ²Ø§Ù… Ø¨Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„ØªÙˆØµÙŠÙ„ ØŒ Ø§Ù„Ø·Ù„Ø¨ ÙŠØ¬Ù„Ø³...</td>\n",
       "      <td>y_awdah</td>\n",
       "      <td>1</td>\n",
       "      <td>5.13.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HungerStation</td>\n",
       "      <td>2021-11-02</td>\n",
       "      <td>ÙØ§Ø´Ù„</td>\n",
       "      <td>Ø§Ù‚Ù„ Ù…Ù† Ù†Ø¬Ù…Ù‡ ÙˆÙ†ØµØ§Ø¨ÙŠÙ† ÙˆÙŠØ²ÙŠØ¯ÙˆÙ† ÙÙ„ÙˆØ³ ÙØ¬Ø§Ù‡ Ø¨Ø¯ÙˆÙ† Ø³Ø¨Ø¨...</td>\n",
       "      <td>ii.jem2</td>\n",
       "      <td>1</td>\n",
       "      <td>5.13.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HungerStation</td>\n",
       "      <td>2021-11-02</td>\n",
       "      <td>horrible</td>\n",
       "      <td>canceled my order on me did not provide any in...</td>\n",
       "      <td>Dead Rat2</td>\n",
       "      <td>1</td>\n",
       "      <td>5.13.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Source        Date                     Title  \\\n",
       "0  HungerStation  2021-11-02            Ø§ØªÙ…Ù†Ù‰ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡   \n",
       "1  HungerStation  2021-11-02  Ø§Ø¹Ù„Ø§Ù†Ø§ØªÙ‡Ù… Ø£ÙƒØ¨Ø± Ù…Ù† ÙˆØ§Ù‚Ø¹Ù‡Ù…   \n",
       "2  HungerStation  2021-11-02                   Ø³ÙŠØ¡ Ø¬Ø¯Ø§   \n",
       "3  HungerStation  2021-11-02                      ÙØ§Ø´Ù„   \n",
       "4  HungerStation  2021-11-02                  horrible   \n",
       "\n",
       "                                             Content       Name  Rating  \\\n",
       "0  ØªØºÙ„ÙŠÙ Ø§Ù„Ù…Ø´Ø±ÙˆØ¨Ø§Øª Ø¨Ø§Ø­ÙƒØ§Ù… (Ø§Ù„Ø­Ø§Ø± ÙˆØ§Ù„Ø¨Ø§Ø±Ø¯) Ù„Ù„ØªØ§ÙƒØ¯ ...    'Ø£Ù…ÙŠØ±Ø©'       2   \n",
       "1                                           Ø¬Ø§Ù‡Ø² ÙˆØ¨Ø³     Ø¯Ø­ 911       1   \n",
       "2  Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø§ÙŠ Ø§Ù„ØªØ²Ø§Ù… Ø¨Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„ØªÙˆØµÙŠÙ„ ØŒ Ø§Ù„Ø·Ù„Ø¨ ÙŠØ¬Ù„Ø³...    y_awdah       1   \n",
       "3  Ø§Ù‚Ù„ Ù…Ù† Ù†Ø¬Ù…Ù‡ ÙˆÙ†ØµØ§Ø¨ÙŠÙ† ÙˆÙŠØ²ÙŠØ¯ÙˆÙ† ÙÙ„ÙˆØ³ ÙØ¬Ø§Ù‡ Ø¨Ø¯ÙˆÙ† Ø³Ø¨Ø¨...    ii.jem2       1   \n",
       "4  canceled my order on me did not provide any in...  Dead Rat2       1   \n",
       "\n",
       "  Version  \n",
       "0  5.13.2  \n",
       "1  5.13.2  \n",
       "2  5.13.2  \n",
       "3  5.13.2  \n",
       "4  5.13.2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ØªØºÙ„ÙŠÙ Ø§Ù„Ù…Ø´Ø±ÙˆØ¨Ø§Øª Ø¨Ø§Ø­ÙƒØ§Ù… (Ø§Ù„Ø­Ø§Ø± ÙˆØ§Ù„Ø¨Ø§Ø±Ø¯) Ù„Ù„ØªØ§ÙƒØ¯ Ù…Ù† Ø§Ù† Ø§Ù„Ù…Ù†Ø¯ÙˆØ¨ Ù„Ù… Ù„Ù… ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø´Ø±ÙˆØ¨'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Content'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thereâ€™s is ZERO customer service. I never received an order, was charged for it and then was never refunded. To add insult to injury they marked my issue as resolved. I cannot get through to them anymore. Absolutely disgraceful and dishonest. Avoid this crooked company like the plague. If I could give -10 stars I would. Utterly horrid!\n"
     ]
    }
   ],
   "source": [
    "print (df['Content'][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø§ÙØ¶Ù„ Ù…Ù† Ù…Ø±Ø³ÙˆÙ„ ğŸ¤ğŸ¤\n"
     ]
    }
   ],
   "source": [
    "print (df['Content'][13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it first time only, to install the library.\n",
    "#!pip install camel-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arabic Preprocessing Pipeline\n",
    "There are many tools for processing Arabic text: NLTK, Gensim, Farasa, MADAMIRA and Stanford CoreNLP, CAMeL. We will use CAMeL, NLTK, and Gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Dediacritization (Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:  Ø¥ÙÙ†Ù‘Ù Ø§Ù„Ù„Ù‘ÙÙ‡Ù ÙˆÙÙ…ÙÙ„ÙØ§Ø¦ÙÙƒÙØªÙÙ‡Ù ÙŠÙØµÙÙ„Ù‘ÙÙˆÙ†Ù Ø¹ÙÙ„ÙÙ‰ Ø§Ù„Ù†Ù‘ÙØ¨ÙÙŠÙ‘Ù Ûš ÙŠÙØ§ Ø£ÙÙŠÙ‘ÙÙ‡ÙØ§ Ø§Ù„Ù‘ÙØ°ÙÙŠÙ†Ù Ø¢Ù…ÙÙ†ÙÙˆØ§ ØµÙÙ„Ù‘ÙÙˆØ§ Ø¹ÙÙ„ÙÙŠÙ’Ù‡Ù ÙˆÙØ³ÙÙ„Ù‘ÙÙ…ÙÙˆØ§ ØªÙØ³Ù’Ù„ÙÙŠÙ…Ù‹Ø§ \n",
      "after :  Ø¥Ù† Ø§Ù„Ù„Ù‡ ÙˆÙ…Ù„Ø§Ø¦ÙƒØªÙ‡ ÙŠØµÙ„ÙˆÙ† Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø¨ÙŠ Ûš ÙŠØ§ Ø£ÙŠÙ‡Ø§ Ø§Ù„Ø°ÙŠÙ† Ø¢Ù…Ù†ÙˆØ§ ØµÙ„ÙˆØ§ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù…ÙˆØ§ ØªØ³Ù„ÙŠÙ…Ø§ \n"
     ]
    }
   ],
   "source": [
    "# import the dediacritization tool\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "\n",
    "text  = \"Ø¥ÙÙ†Ù‘Ù Ø§Ù„Ù„Ù‘ÙÙ‡Ù ÙˆÙÙ…ÙÙ„ÙØ§Ø¦ÙÙƒÙØªÙÙ‡Ù ÙŠÙØµÙÙ„Ù‘ÙÙˆÙ†Ù Ø¹ÙÙ„ÙÙ‰ Ø§Ù„Ù†Ù‘ÙØ¨ÙÙŠÙ‘Ù Ûš ÙŠÙØ§ Ø£ÙÙŠÙ‘ÙÙ‡ÙØ§ Ø§Ù„Ù‘ÙØ°ÙÙŠÙ†Ù Ø¢Ù…ÙÙ†ÙÙˆØ§ ØµÙÙ„Ù‘ÙÙˆØ§ Ø¹ÙÙ„ÙÙŠÙ’Ù‡Ù ÙˆÙØ³ÙÙ„Ù‘ÙÙ…ÙÙˆØ§ ØªÙØ³Ù’Ù„ÙÙŠÙ…Ù‹Ø§ \"\n",
    "text2 = dediac_ar(text)\n",
    "print (\"before: \", text)\n",
    "print (\"after : \", text2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Normalizing Alif and Tah Marbotah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel_tools.utils.normalize import normalize_alef_maksura_ar\n",
    "from camel_tools.utils.normalize import normalize_alef_ar\n",
    "from camel_tools.utils.normalize import normalize_teh_marbuta_ar\n",
    "\n",
    "def ortho_normalize(text):\n",
    "    text = normalize_alef_maksura_ar(text)\n",
    "    text = normalize_alef_ar(text)\n",
    "    text = normalize_teh_marbuta_ar(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:  Ø§Ù„Ø£Ù… Ø§Ù„Ø¢Ù„Ø§Ù… Ø§Ù„Ø¥Ù…Ø±Ø£Ø©\n",
      "after :  Ø§Ù„Ø§Ù… Ø§Ù„Ø§Ù„Ø§Ù… Ø§Ù„Ø§Ù…Ø±Ø§Ù‡\n"
     ]
    }
   ],
   "source": [
    "text  = \"Ø§Ù„Ø£Ù… Ø§Ù„Ø¢Ù„Ø§Ù… Ø§Ù„Ø¥Ù…Ø±Ø£Ø©\"\n",
    "text2 = ortho_normalize(text)\n",
    "print (\"before: \", text)\n",
    "print (\"after : \", text2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Remove English Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_english(text):\n",
    "    text = re.sub(r'\\s*[A-Za-z]+\\b', '' , text)\n",
    "    return (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:  Thereâ€™s is ZERO customer service. I never received an order, was charged for it and then was never refunded. To add insult to injury they marked my issue as resolved. I cannot get through to them anymore. Absolutely disgraceful and dishonest. Avoid this crooked company like the plague. If I could give -10 stars I would. Utterly horrid!\n",
      "after :  â€™.,..... -10.!\n"
     ]
    }
   ],
   "source": [
    "text = df['Content'][9]\n",
    "text2 = remove_english(text)\n",
    "print (\"before: \", text)\n",
    "print (\"after : \", text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    # define a list of arabic and english punctiations that we want to get rid of in our text\n",
    "    punctuations = '''`Ã·Ã—Ø›<>_()*&^%][Ù€ØŒ/:\"ØŸ.,'{}~Â¦+|!â€â€¦â€œâ€“Ù€''' + string.punctuation\n",
    "    translator = str.maketrans('', '', punctuations)\n",
    "    text = text.translate(translator)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- maketrans( ) method creates a one to one mapping of a character to its translation/replacement.\n",
    "- translate( ) method makes a copy of a string with a specific set of values substituted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:  Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø§ÙŠ Ø§Ù„ØªØ²Ø§Ù… Ø¨Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„ØªÙˆØµÙŠÙ„ ØŒ Ø§Ù„Ø·Ù„Ø¨ ÙŠØ¬Ù„Ø³ Ø³Ø§Ø¹ØªÙŠÙ† Ù„ÙŠÙ† ÙŠØ¬ÙŠÙƒ ! ÙˆÙŠØ¬ÙŠ Ø¨Ø§Ø±Ø¯ ÙˆØ§Ù„Ù…Ù†Ø¯ÙˆØ¨ ÙŠØ§Ù„Ù„Ù‡ ÙŠØ±Ø¯ Ù…Ø§ ØªØ·ÙˆØ± ÙˆÙ„Ø§ Ø´ÙŠ Ø¨Ù‡Ù†Ù‚Ø±Ø³ØªÙŠØ´Ù† Ù…Ù† ÙŠÙˆÙ… Ø·Ù„Ø¹ Ù„Ù„Ø­ÙŠÙ† Ù„ÙŠØªÙ‡Ù… ÙŠØªØ¹Ù„Ù…ÙˆÙ† Ù…Ù† Ø¬Ø§Ù‡Ø² Ø¨Ø³\n",
      "after :  Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø§ÙŠ Ø§Ù„ØªØ²Ø§Ù… Ø¨Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„ØªÙˆØµÙŠÙ„  Ø§Ù„Ø·Ù„Ø¨ ÙŠØ¬Ù„Ø³ Ø³Ø§Ø¹ØªÙŠÙ† Ù„ÙŠÙ† ÙŠØ¬ÙŠÙƒ  ÙˆÙŠØ¬ÙŠ Ø¨Ø§Ø±Ø¯ ÙˆØ§Ù„Ù…Ù†Ø¯ÙˆØ¨ ÙŠØ§Ù„Ù„Ù‡ ÙŠØ±Ø¯ Ù…Ø§ ØªØ·ÙˆØ± ÙˆÙ„Ø§ Ø´ÙŠ Ø¨Ù‡Ù†Ù‚Ø±Ø³ØªÙŠØ´Ù† Ù…Ù† ÙŠÙˆÙ… Ø·Ù„Ø¹ Ù„Ù„Ø­ÙŠÙ† Ù„ÙŠØªÙ‡Ù… ÙŠØªØ¹Ù„Ù…ÙˆÙ† Ù…Ù† Ø¬Ø§Ù‡Ø² Ø¨Ø³\n"
     ]
    }
   ],
   "source": [
    "text = df['Content'][2]\n",
    "text2 = remove_punc(text)\n",
    "print (\"before: \", text)\n",
    "print (\"after : \", text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Remove emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:  Ø§ÙØ¶Ù„ Ù…Ù† Ù…Ø±Ø³ÙˆÙ„ ğŸ¤ğŸ¤\n",
      "after :  Ø§ÙØ¶Ù„ Ù…Ù† Ù…Ø±Ø³ÙˆÙ„ \n"
     ]
    }
   ],
   "source": [
    "# you have to install this first by: !pip install demoji\n",
    "\n",
    "import demoji\n",
    "text = df['Content'][13]\n",
    "text2 = demoji.replace(text, '')\n",
    "print (\"before: \", text)\n",
    "print (\"after : \", text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:  Ø¥ÙÙ†Ù‘Ù Ø§Ù„Ù„Ù‘ÙÙ‡Ù ÙˆÙÙ…ÙÙ„ÙØ§Ø¦ÙÙƒÙØªÙÙ‡Ù ÙŠÙØµÙÙ„Ù‘ÙÙˆÙ†Ù Ø¹ÙÙ„ÙÙ‰ Ø§Ù„Ù†Ù‘ÙØ¨ÙÙŠÙ‘Ù Ûš ÙŠÙØ§ Ø£ÙÙŠÙ‘ÙÙ‡ÙØ§ Ø§Ù„Ù‘ÙØ°ÙÙŠÙ†Ù Ø¢Ù…ÙÙ†ÙÙˆØ§ ØµÙÙ„Ù‘ÙÙˆØ§ Ø¹ÙÙ„ÙÙŠÙ’Ù‡Ù ÙˆÙØ³ÙÙ„Ù‘ÙÙ…ÙÙˆØ§ ØªÙØ³Ù’Ù„ÙÙŠÙ…Ù‹Ø§ \n",
      "after :  ['Ø§Ù†', 'Ø§Ù„Ù„Ù‡', 'ÙˆÙ…Ù„Ø§Ø¦ÙƒØªÙ‡', 'ÙŠØµÙ„ÙˆÙ†', 'Ø¹Ù„ÙŠ', 'Ø§Ù„Ù†Ø¨ÙŠ', 'Ûš', 'ÙŠØ§', 'Ø§ÙŠÙ‡Ø§', 'Ø§Ù„Ø°ÙŠÙ†', 'Ø§Ù…Ù†ÙˆØ§', 'ØµÙ„ÙˆØ§', 'Ø¹Ù„ÙŠÙ‡', 'ÙˆØ³Ù„Ù…ÙˆØ§', 'ØªØ³Ù„ÙŠÙ…Ø§']\n"
     ]
    }
   ],
   "source": [
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "\n",
    "text  = \"Ø¥ÙÙ†Ù‘Ù Ø§Ù„Ù„Ù‘ÙÙ‡Ù ÙˆÙÙ…ÙÙ„ÙØ§Ø¦ÙÙƒÙØªÙÙ‡Ù ÙŠÙØµÙÙ„Ù‘ÙÙˆÙ†Ù Ø¹ÙÙ„ÙÙ‰ Ø§Ù„Ù†Ù‘ÙØ¨ÙÙŠÙ‘Ù Ûš ÙŠÙØ§ Ø£ÙÙŠÙ‘ÙÙ‡ÙØ§ Ø§Ù„Ù‘ÙØ°ÙÙŠÙ†Ù Ø¢Ù…ÙÙ†ÙÙˆØ§ ØµÙÙ„Ù‘ÙÙˆØ§ Ø¹ÙÙ„ÙÙŠÙ’Ù‡Ù ÙˆÙØ³ÙÙ„Ù‘ÙÙ…ÙÙˆØ§ ØªÙØ³Ù’Ù„ÙÙŠÙ…Ù‹Ø§ \"\n",
    "text2 = dediac_ar(text)\n",
    "text2 = ortho_normalize(text2)\n",
    "tokenized = simple_word_tokenize(text2)\n",
    "print (\"before: \", text)\n",
    "print (\"after : \", tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Stop-words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = nltk.corpus.stopwords.words(\"arabic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ø§Ù†',\n",
       " 'Ø§Ù„Ù„Ù‡',\n",
       " 'ÙˆÙ…Ù„Ø§Ø¦ÙƒØªÙ‡',\n",
       " 'ÙŠØµÙ„ÙˆÙ†',\n",
       " 'Ø¹Ù„ÙŠ',\n",
       " 'Ø§Ù„Ù†Ø¨ÙŠ',\n",
       " 'Ûš',\n",
       " 'Ø§ÙŠÙ‡Ø§',\n",
       " 'Ø§Ù…Ù†ÙˆØ§',\n",
       " 'ØµÙ„ÙˆØ§',\n",
       " 'ÙˆØ³Ù„Ù…ÙˆØ§',\n",
       " 'ØªØ³Ù„ÙŠÙ…Ø§']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text = [word for word in tokenized if word not in stop_words]\n",
    "clean_text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Stemming or Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø­Ø±Ùƒ\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.isri import ISRIStemmer\n",
    "st = ISRIStemmer()\n",
    "w = 'Ø­Ø±ÙƒØ§Øª'\n",
    "print(st.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other preprocessing include stemming, removing numbers, removing URLs etc, it all depends on the task you need to perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put them all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    \n",
    "    # if you need to use this function in other code or notebook \n",
    "    # make sure you import the needed libraries.\n",
    "    \n",
    "    cleaned = dediac_ar(text)\n",
    "    cleaned = ortho_normalize(cleaned)\n",
    "    cleaned = remove_english(cleaned)\n",
    "    cleaned = remove_punc(cleaned)\n",
    "    cleaned = demoji.replace(cleaned, '')\n",
    "    cleaned = tokenized = simple_word_tokenize(cleaned)\n",
    "    cleaned = [word for word in tokenized if word not in stop_words]\n",
    "    cleaned = [st.stem(word) for word in cleaned]\n",
    "    return \" \".join(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ø·Ù„Ø¨ Ø¬Ù„Ø³ Ø³Ø§Ø¹ Ù„ÙŠÙ† ÙŠØ¬Ùƒ ÙØ¶Ù„ Ø±Ø³Ù„ Ø§Ø´ÙŠ'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \" Ø§Ù„Ø·Ù„Ø¨ ÙŠØ¬Ù„Ø³ Ø³Ø§Ø¹ØªÙŠÙ† Ù„ÙŠÙ† ÙŠØ¬ÙŠÙƒ !Ù„ÙƒÙ† Ø§ÙØ¶Ù„ Ù…Ù† Ù…Ø±Ø³ÙˆÙ„ ğŸ‘ğŸ¼ğŸ‘ğŸ¼ Ù…ÙØ§Ø´ÙÙŠ \"\n",
    "text2 = clean(text)\n",
    "text2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply it to all the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "      <th>Name</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HungerStation</td>\n",
       "      <td>2021-11-02</td>\n",
       "      <td>Ø§ØªÙ…Ù†Ù‰ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡</td>\n",
       "      <td>ØºÙ„Ù Ø´Ø±Ø¨ Ø­ÙƒÙ… Ø­Ø§Ø± Ø¨Ø±Ø¯ ØªÙƒØ¯ Ø§Ù† Ù†Ø¯Ø¨ Ø®Ø¯Ù… Ø´Ø±Ø¨</td>\n",
       "      <td>'Ø£Ù…ÙŠØ±Ø©'</td>\n",
       "      <td>2</td>\n",
       "      <td>5.13.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HungerStation</td>\n",
       "      <td>2021-11-02</td>\n",
       "      <td>Ø§Ø¹Ù„Ø§Ù†Ø§ØªÙ‡Ù… Ø£ÙƒØ¨Ø± Ù…Ù† ÙˆØ§Ù‚Ø¹Ù‡Ù…</td>\n",
       "      <td>Ø¬Ù‡Ø² ÙˆØ¨Ø³</td>\n",
       "      <td>Ø¯Ø­ 911</td>\n",
       "      <td>1</td>\n",
       "      <td>5.13.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HungerStation</td>\n",
       "      <td>2021-11-02</td>\n",
       "      <td>Ø³ÙŠØ¡ Ø¬Ø¯Ø§</td>\n",
       "      <td>ÙˆØ¬Ø¯ Ø§ÙŠ ØªØ²Ù… Ù…ÙˆØ§Ø¹ÙŠØ¯ ÙˆØµÙ„ Ø·Ù„Ø¨ Ø¬Ù„Ø³ Ø³Ø§Ø¹ Ù„ÙŠÙ† ÙŠØ¬Ùƒ ÙˆÙŠØ¬ ...</td>\n",
       "      <td>y_awdah</td>\n",
       "      <td>1</td>\n",
       "      <td>5.13.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HungerStation</td>\n",
       "      <td>2021-11-02</td>\n",
       "      <td>ÙØ§Ø´Ù„</td>\n",
       "      <td>Ø§Ù‚Ù„ Ù†Ø¬Ù… Ù†ØµØ¨ ÙŠØ²Ø¯ ÙÙ„Ø³ ÙØ¬Ù‡ Ø¨Ø¯Ù† Ø³Ø¨Ø¨ Ù‚Ø¦Ù… Ø³Ø¹Ø± Ùˆ Ø³Ù„Ù‡ ...</td>\n",
       "      <td>ii.jem2</td>\n",
       "      <td>1</td>\n",
       "      <td>5.13.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HungerStation</td>\n",
       "      <td>2021-11-02</td>\n",
       "      <td>horrible</td>\n",
       "      <td></td>\n",
       "      <td>Dead Rat2</td>\n",
       "      <td>1</td>\n",
       "      <td>5.13.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Source        Date                     Title  \\\n",
       "0  HungerStation  2021-11-02            Ø§ØªÙ…Ù†Ù‰ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡   \n",
       "1  HungerStation  2021-11-02  Ø§Ø¹Ù„Ø§Ù†Ø§ØªÙ‡Ù… Ø£ÙƒØ¨Ø± Ù…Ù† ÙˆØ§Ù‚Ø¹Ù‡Ù…   \n",
       "2  HungerStation  2021-11-02                   Ø³ÙŠØ¡ Ø¬Ø¯Ø§   \n",
       "3  HungerStation  2021-11-02                      ÙØ§Ø´Ù„   \n",
       "4  HungerStation  2021-11-02                  horrible   \n",
       "\n",
       "                                             Content       Name  Rating  \\\n",
       "0             ØºÙ„Ù Ø´Ø±Ø¨ Ø­ÙƒÙ… Ø­Ø§Ø± Ø¨Ø±Ø¯ ØªÙƒØ¯ Ø§Ù† Ù†Ø¯Ø¨ Ø®Ø¯Ù… Ø´Ø±Ø¨    'Ø£Ù…ÙŠØ±Ø©'       2   \n",
       "1                                            Ø¬Ù‡Ø² ÙˆØ¨Ø³     Ø¯Ø­ 911       1   \n",
       "2  ÙˆØ¬Ø¯ Ø§ÙŠ ØªØ²Ù… Ù…ÙˆØ§Ø¹ÙŠØ¯ ÙˆØµÙ„ Ø·Ù„Ø¨ Ø¬Ù„Ø³ Ø³Ø§Ø¹ Ù„ÙŠÙ† ÙŠØ¬Ùƒ ÙˆÙŠØ¬ ...    y_awdah       1   \n",
       "3  Ø§Ù‚Ù„ Ù†Ø¬Ù… Ù†ØµØ¨ ÙŠØ²Ø¯ ÙÙ„Ø³ ÙØ¬Ù‡ Ø¨Ø¯Ù† Ø³Ø¨Ø¨ Ù‚Ø¦Ù… Ø³Ø¹Ø± Ùˆ Ø³Ù„Ù‡ ...    ii.jem2       1   \n",
       "4                                                     Dead Rat2       1   \n",
       "\n",
       "  Version  \n",
       "0  5.13.2  \n",
       "1  5.13.2  \n",
       "2  5.13.2  \n",
       "3  5.13.2  \n",
       "4  5.13.2  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = df\n",
    "new_df['Content'] = df['Content'].apply(clean)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These preprocessing step are needed for machine learning algoriyhms. Now you can apply sentiment analysis as we did in English NLP, but you need a labelled dataset. This one is not labelled. You can label it, but it is too small. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, for text summarization we would not need all the preprocessing steps above. We will see an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Summarization Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use an algorithm called TextRank, which depends on PageRank algorithm. \n",
    "- TextRank implementation is available at Gensim library. \n",
    "- PageRank algorithm first developed by Google co-founder Larry Page. \n",
    "- The algorithm rank pages by importance, the most important page is the page that receives the largest number of links from â€œalsoâ€ important pages.\n",
    "- It depends on graph theory as it considers a web page as a node and the edges are links (hyperlinks) between pages.\n",
    "- In TextRank, node represent sentences and edges represents similarities between sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The higher the PageRank of a link, the more authoritative it is. \n",
    "We can simplify the PageRank algorithm to describe it as a way for the importance of a webpage to be measured by analyzing the quantity and quality of the links that point to it.\" [2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"TextRank â€“ is a graph-based ranking model for text processing which can be used in order to find the most relevant sentences in text and also to find keywords. The algorithm is explained in detail in the paper at https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf\" [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim python library include an implmentation of TextRank. Let's try it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will combine all the sentences together as one long sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"hungerstation.csv\")\n",
    "sentences = ''\n",
    "for s in df['Content']:\n",
    "    sentences = sentences + s + '. '\n",
    "\n",
    "#sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will import the library that will do all the work for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextRank\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø§ÙŠ Ø§Ù„ØªØ²Ø§Ù… Ø¨Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„ØªÙˆØµÙŠÙ„ ØŒ Ø§Ù„Ø·Ù„Ø¨ ÙŠØ¬Ù„Ø³ Ø³Ø§Ø¹ØªÙŠÙ† Ù„ÙŠÙ† ÙŠØ¬ÙŠÙƒ !\n",
      "my money is lost and every time i try to contact customer service it gives me a â€œsomething went wrongâ€ message \n",
      "They always demand that we come down to take the order even though the delivery charge is very high already.\n",
      "It is only normal that the order is delivered to the doorstep; that is what we pay for after all with the delivery charge.\n",
      "ÙÙŠ Ø§Ù„Ø·Ø§ÙŠÙ Ø§Ù„Ù…Ø­Ù„Ø§Øª ÙˆØ§Ù„Ù…Ø·Ø§Ø¹Ù… Ù…Ø´ØºÙˆÙ„Ø© Ø§Ùˆ Ù…ØºÙ„Ù‚Ø© Ø§Ø¯Ø®Ù„ Ù…Ø±Ø³ÙˆÙ„ ÙƒÙ„Ù‡ Ù…ÙØªÙˆØ­Ø© Ø§ÙŠØ´ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ÙƒÙ„ Ù…Ø±Ù‡ Ø²ÙŠ ÙƒØ°Ø§ ÙŠ Ù…ØºÙ„Ù‚ Ø§Ùˆ Ù…Ø´ØºÙˆÙ„ ÙƒØ±Ù‡Øª Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù…Ù† Ø²Ù…Ø§Ù† Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ¶Ø¹ Ø°Ø§ ÙˆÙ„Ø§ ØªØºÙŠØ±.\n",
      "Itâ€™s worse it application delivery food  I have tried in Saudi Arabia.\n",
      "Ø³Ø­Ø¨Ùˆ Ø§Ù„ÙÙ„ÙˆØ³ Ùˆ ØªÙ… Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ø·Ù„Ø¨ ÙˆÙ„Ø§ ÙŠÙˆØ¬Ø¯ Ø·Ø±ÙŠÙ‚Ø© ØªÙˆØ§ØµÙ„ ÙˆÙ„Ù… ÙŠØªÙ… Ø¥Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø¨Ù„Øº.\n",
      "ÙŠØ¹Ù†ÙŠ ÙŠØ§Ø®Ø° Ø·Ù„Ø¨Ùƒ ÙˆÙŠØ±ÙˆØ­ Ù…Ø·Ø¹Ù… Ø«Ø§Ù†ÙŠØŒ ÙŠÙ†ØªØ¸Ø± Ø§Ù„Ø·Ù„Ø¨ Ø¨Ø¹Ø¯ÙŠÙ† ÙŠØ¬ÙŠÙƒ Ø§Ù„Ø§ÙƒÙ„ Ø¨Ø§Ø±Ø¯.\n",
      "Ø§Ù„Ù…Ù†Ø¯ÙˆØ¨ ÙŠÙ‚Ø¨Ù„ Ø§Ù„Ø·Ù„Ø¨ ÙˆÙ‡Ùˆ Ù…Ø§Ù…Ø¹Ù‡ ÙÙ„ÙˆØ³.\n",
      "Ø§Ø³ÙˆØ¡ ØªØ·Ø¨ÙŠÙ‚ ØªÙˆØµÙŠÙ„ Ù…Ù…ÙƒÙ† ØªØ´ÙˆÙÙ‡ Ø§Ù„Ø§ÙƒÙ„ ÙŠÙˆØµÙ„ Ø¨Ø§Ø±Ø¯ Ùˆ Ù†Øµ Ø§Ù„Ø§ÙƒÙ„ Ù…Ø§ÙƒÙˆÙ„ Ø§Ù„Ù„Ù‡ Ù„Ø§ ÙŠØ³Ø§Ù…Ø­Ù‡Ù… Ùˆ ØªÙˆØµÙŠÙ„Ù‡Ù… Ø§ØºÙ„Ù‰ Ù…Ù† Ø§Ù„Ø·Ù„Ø¨ Ù†ÙØ³Ù‡.\n",
      "Best app as well as the service they provide incredible..\n",
      "ÙƒÙ„ ÙˆØ¬Ø¨Ø© ÙŠØ²ÙŠØ¯ Ø³Ø¹Ø±Ù‡Ø§ ÙÙŠ Ù‡Ù†Ù‚Ø±Ø³ØªÙŠØ´Ù† Ø¨Ù…Ø§ Ù„Ø§ ÙŠÙ‚Ù„ Ø¹Ù† Ù£ Ø±ÙŠØ§Ù„ ÙˆØªÙˆØµÙ„ Ø§Ù„Ù‰ Ù¨ Ø±ÙŠØ§Ù„â€¦ ÙˆÙÙˆÙ‚ ÙƒØ°Ø§ ÙŠØªØ£Ø®Ø± Ø§Ù„Ù…Ù†Ø¯ÙˆØ¨ ÙˆÙŠÙˆØµÙ„Ùƒ Ø§Ù„Ø§ÙƒÙ„ Ø¨Ø§Ø±Ø¯.\n",
      "Once the order delivered, this app will be deleted for good..\n",
      "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§ÙˆÙ„Ø§Ù‹ Ø§Ù„Ù…Ø¹Ø±ÙˆÙ ÙÙŠÙ†ÙŠ Ù‡Ø§Ø¯Ø¦ ÙˆÙ…Ø§ Ø§Ø¯Ù‚Ù‚ Ø¨Ø§Ø´ÙŠØ¦ ÙƒØ«ÙŠØ± Ø¨Ø³ Ù„Ù„Ø§Ù…Ø§Ù†Ø© Ø±ÙØ¹ Ø¶ØºØ·ÙŠ Ø³ÙŠØ¦ Ø¬Ø¯Ø§Ù‹ ØªØ£Ø®ÙŠØ± ØºÙŠØ± Ø·Ø¨ÙŠØ¹ÙŠ ÙŠÙ…Ø¯ÙŠÙƒ ØªØ±ÙˆØ­ Ù…Ø´ÙŠ ÙˆØªØ±Ø¬Ø¹ ÙˆÙ‡Ùˆ Ù…Ø§Ø®Ù„Øµ ØªØ¹Ù„ÙŠÙ‚ Ø¨Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙˆØ§Ù„Ù…Ù†Ø¯ÙˆØ¨ Ø§ØªÙˆÙ‚Ø¹ ÙˆØ§Ø­Ø¯ Ø¨Ø³ ÙŠÙ„Ù Ø§Ù„Ø·Ø§Ø¦Ù ÙƒÙ„Ù‡Ø§ ÙˆØ¨Ø¹Ø¯ÙŠÙ† ÙŠØ¬ÙŠÙƒ Ø§ØªÙ…Ù†Ù‰ ÙŠÙ„Ù‚Ùˆ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬\n",
      "Ø§Ù„Ø·Ù„Ø¨ Ø§Ø®Ø° Ø³Ø§Ø¹ØªÙŠÙ† Ù‡Ø°Ø§ ØºÙŠØ± Ù…Ø¹Ù‚ÙˆÙ„ ÙˆØ§Ù„Ù…ÙˆØ¸Ù Ù„Ø§ÙŠØªØ¬Ø§ÙˆØ¨ ÙˆÙŠÙ‚ÙˆÙ„ Ø§Ù†Ø§ Ù„Ø§ Ø§Ø¯Ø±ÙŠ Ø·Ù„Ø¨Ùƒ Ø¹Ù†Ø¯ Ù…Ù€Ù† ØŸØŸØŸØŸØŸ\n",
      "Ø·Ù„Ø¨Øª Ø§ÙƒØ«Ø± Ù…Ù† Ù…Ø±Ù‡ Ù…Ù† Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙˆØ§Ù„Ù…Ø´ÙƒÙ„Ù‡ Ø§Ù„Ø·Ù„Ø¨ ÙŠØ§ØµÙ„ Ù…ØªØ£Ø®Ø± ÙˆØ¨Ø§Ø±Ø¯ ÙˆÙ‚Ù„ÙŠÙ„ÙŠÙ† ÙÙ‡Ù…Ù… \n",
      "Ù„ÙŠØ´ Ø¬Ø§Ù†ÙŠ Ø®ØµÙ… 66 Ø±ÙŠØ§Ù„ Ù‚ÙŠÙ…Ø© Ø§Ù„Ø·Ù„Ø¨ ÙˆØ§Ù„ØªÙˆØµÙŠÙ„\n",
      "ØªØ·Ø¨ÙŠÙ‚ Ù…Ù† Ø³ÙŠØ¡ Ø§Ù„Ù‰ Ø§Ø³ÙˆØ§Ø¡ â€¦ Ø§Ù„Ø³Ø¹Ø± Ø§ØºÙ„Ù‰ â€¦ Ø¯Ø§ÙŠÙ… ÙŠØªØ§Ø®Ø± ÙˆØ¯Ø§ÙŠÙ… Ø§Ù„Ø§ÙƒÙ„ ÙŠØ¬ÙŠ Ø¨Ø§Ø±Ø¯.\n",
      "Ø§Ø³Ø¹Ø§Ø± Ø§Ù„ØªÙˆØµÙŠÙ„ Ø¹Ù†Ø¯ÙƒÙ… Ø§ØºÙ„Ù‰ Ù…Ù† Ø§Ù„Ø§ÙƒÙ„ Ù†ÙØ³Ù‡ ÙˆØºÙŠØ± Ù‡Ø°Ø§ Ù…Ù†Ø§Ø¯ÙŠØ¨ÙƒÙ… ÙŠØªØ§Ø®Ø±ÙˆÙ† Ø¯Ø§ÙŠÙ… ÙŠØ§Ù„ÙŠØª ØªØ±Ø§Ø¬Ø¹ÙˆÙ† Ø§Ø³Ø¹Ø§Ø± Ø§Ù„ØªÙˆØµÙŠÙ„ !.\n",
      "Itâ€™s a really good app but it does lag from time to time and also the delivery fees are kinda expensive 15 SAR so my order can be delivered if the delivery can be a little cheaper then I would 5 stars but for now I will give it a 4.\n",
      "Ù…Ø§ØªÙ‚Ø¯Ø± ØªØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø®Ø¯Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ ÙˆÙ„Ø§ÙŠÙÙŠØ¯ÙˆÙ†Ùƒ Ø¨Ø´ÙŠ ÙˆØ§Ù„Ø·Ù„Ø¨ ÙŠØ¬ÙŠÙƒ Ø¨Ø¹Ø¯ Ù…Ø§ÙŠØ±ÙˆØ­ Ø§Ù„Ù…Ù†Ø¯ÙˆØ¨ Ù„Ø§ÙƒØ«Ø± Ù…Ù† Ø²Ø¨ÙˆÙ† ÙˆÙŠØ¬ÙŠÙƒ Ø¨Ø§Ø±Ø¯.\n",
      "Ø§ØºÙ„Ø¨ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª ØªØ³Ø¨ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¨Ù„Ø¹ÙƒØ³ ÙƒÙ„ Ø´ÙˆÙŠ Ø§Ø·Ù„Ø¨ Ù…Ù†Ù‡ Ù…Ùˆ Ø§ÙˆÙ„ Ù…Ø±Ù‡ ØŒÙ…Ø±Ù‡ Ø·Ù„Ø¨Øª Ø­Ù„Ø§ ÙˆØ¯ÙˆÙ†Ø§Øª ÙˆÙ…Ù† Ø¹ØµØ§Ø¦Ø± ÙˆÙ…Ù† Ù…Ø§Ùƒ ÙˆÙŠÙˆØµÙ„ Ø¨Ø³Ø±Ø¹Ø© Ù…Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡ Ø¨Ø³ Ø¨Ø¹Ø¶ Ø§Ù„Ø§Ø­ÙŠØ§Ù† ÙŠÙƒÙˆÙ† Ø¶ØºØ· Ø¹Ù†Ø¯Ù‡Ù….\n",
      "Ø³Ù„Ø§Ù…Ø§Øª Ù…Ø§Ù‚Ø¯Ø± Ø§Ù„ØºÙŠ Ø§Ù„Ø·Ù„Ø¨ ÙØ±Ø¶Ø§ Ø·Ù„Ø¨Øª Ø¨Ø§Ù„ØºÙ„Ø· ÙˆØ­Ø¨ÙŠØª Ø§Ù„ØºÙŠ Ø¨Ù†ÙØ³ Ø§Ù„ÙˆÙ‚Øª ØµØ±Ø§Ø­Ø© ØªØ·Ø¨ÙŠÙ‚ Ø³ÙŠØ¡ !!!!.\n",
      "Ø§Ù„Ù…Ù†Ø¯ÙˆØ¨ Ù‚Ø§Ù„ Ø§Ù†ÙŠ ÙˆØµÙ„Øª Ø§Ù„Ø·Ù„Ø¨ ÙˆÙ„Ù… ÙŠØªÙ… ØªÙˆØµÙŠÙ„Ù‡ \n",
      "Ø§Ø®Ø± Ø§Ù„Ù„ÙŠÙ„ Ù†ØµÙŠØ­Ù‡ Ù„Ø§ ØªØ·Ù„Ø¨ Ø¹Ù†Ø¯ÙƒÙ… Ø¬Ø§Ù‡Ø² ÙˆØ¹Ù†Ø¯ÙƒÙ… Ø§ÙƒØ«Ø± Ù…Ù† ØªØ·Ø¨ÙŠÙ‚.\n",
      "Ù…Ø§ÙÙŠ Ø§ÙŠ Ù…ØµØ¯Ø§Ù‚ÙŠÙ‡ Ø¨Ø§Ù„Ø§ÙƒÙˆØ§Ø¯ ØŒ ÙƒÙˆØ¯ Ø¬ÙŠØ±Ø§Ù† Ù„Ù„Ø·Ù„Ø¨ Ø§Ù„Ø§ÙˆÙ„ Ø­Ø·ÙŠØªÙ‡ Ùˆ Ø·Ù„Ø¹ Ù„ÙŠ Ø§Ù„Ù…Ø¨Ù„Øº ÙˆÙˆÙ‚Øª Ø§Ù„Ø¯ÙØ¹ Ù…Ø§ Ø§Ù†Ø®ØµÙ… Ø´ÙŠ ØŒ ÙƒÙ„Ù…Øª Ø®Ø¯Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ ÙŠÙ‚ÙˆÙ„ Ù„Ø§Ø²Ù… Ù…Ø§ÙŠÙƒÙˆÙ† Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ØªØ­Ù…Ù„ Ù‚Ø¨Ù„ ÙƒØ°Ø§ Ø¨Ø§Ù„Ø¬ÙˆØ§Ù„ğŸ™‚ \n",
      "After waiting for 45 min the order was cancelled by the app.\n",
      "Ø­Ù‚ÙŠÙ‚Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¬Ø¯Ø§ Ø³ÙŠØ¦ ÙˆÙŠØ§Ø®Ø° Ù…Ø¨Ø§Ù„Øº Ø·Ø§Ø¦Ù„Ø© Ù„Ù„ØªÙˆØµÙŠÙ„ ÙˆØºÙŠØ± Ø¯Ù‚ÙŠÙ‚ ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ø¨Ø¯Ø§ ØŒ ÙˆÙŠÙˆØ¬Ø¯ ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§ÙØ¶Ù„ Ù…Ù†Ù‡ Ù…Ø«Ù„ Ø¬Ø§Ù‡Ø² ÙˆÙ‡Ø°Ø§ Ø§Ø®Ø± Ø·Ù„Ø¨Ø© Ø§Ø·Ù„Ø¨Ù‡ Ù…Ù† Ø¹Ù†Ø¯Ù‡Ù… Ø¨Ø¹Ø¯Ù‡Ø§ Ø¨Ø­Ø°ÙÙ‡.\n",
      "). Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ø³Ø¹Ø§Ø± Ø§Ù„ÙˆØ¬Ø¨Ø§Øª Ø¹Ù†Ø¯Ù‡ Ø§ØºÙ„Ù‰ Ù…Ù† Ø§Ù„Ù…Ø·Ø¹Ù… Ù†ÙØ³Ù‡ ÙŠØ¹Ù†ÙŠ ÙŠØ§Ø®Ø°ÙˆÙ† Ø¹Ù„ÙŠÙƒ Ù…Ø¨Ù„Øº Ø§Ù„ØªÙˆØµÙŠÙ„ ÙˆØ²ÙŠØ§Ø¯Ø© Ø¹Ù„ÙŠÙ‡ Ø±Ø§ÙØ¹ÙŠÙ† Ø³Ø¹Ø± Ø§Ù„ÙˆØ¬Ø¨Ø© Ø¹Ù† Ø³Ø¹Ø±Ù‡Ø§ Ø§Ù„Ø±Ø³Ù…ÙŠ ÙÙŠ Ø§ÙŠ Ù…Ø·Ø¹Ù… Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ù†ØµÙŠØ­Ø© ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„ØªÙˆØµÙŠÙ„ ÙƒØ«ÙŠØ±Ù‡ ÙÙŠÙ‡ Ø§ÙØ¶Ù„ Ù…Ù†Ù‡ Ø¨ Ù¡Ù Ù  Ù…Ø±Ù‡.\n",
      "Worst customer service ever.\n",
      "Delivering a quality service is all we need and you have it HungerStation just please consider all regular customers to have special offers..\n"
     ]
    }
   ],
   "source": [
    "short_summary = summarize(sentences)\n",
    "print(short_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø§ÙŠ Ø§Ù„ØªØ²Ø§Ù… Ø¨Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„ØªÙˆØµÙŠÙ„ ØŒ Ø§Ù„Ø·Ù„Ø¨ ÙŠØ¬Ù„Ø³ Ø³Ø§Ø¹ØªÙŠÙ† Ù„ÙŠÙ† ÙŠØ¬ÙŠÙƒ !\n",
      "my money is lost and every time i try to contact customer service it gives me a â€œsomething went wrongâ€ message \n",
      "It is only normal that the order is delivered to the doorstep; that is what we pay for after all with the delivery charge.\n",
      "ÙÙŠ Ø§Ù„Ø·Ø§ÙŠÙ Ø§Ù„Ù…Ø­Ù„Ø§Øª ÙˆØ§Ù„Ù…Ø·Ø§Ø¹Ù… Ù…Ø´ØºÙˆÙ„Ø© Ø§Ùˆ Ù…ØºÙ„Ù‚Ø© Ø§Ø¯Ø®Ù„ Ù…Ø±Ø³ÙˆÙ„ ÙƒÙ„Ù‡ Ù…ÙØªÙˆØ­Ø© Ø§ÙŠØ´ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ÙƒÙ„ Ù…Ø±Ù‡ Ø²ÙŠ ÙƒØ°Ø§ ÙŠ Ù…ØºÙ„Ù‚ Ø§Ùˆ Ù…Ø´ØºÙˆÙ„ ÙƒØ±Ù‡Øª Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù…Ù† Ø²Ù…Ø§Ù† Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ¶Ø¹ Ø°Ø§ ÙˆÙ„Ø§ ØªØºÙŠØ±.\n",
      "Ø³Ø­Ø¨Ùˆ Ø§Ù„ÙÙ„ÙˆØ³ Ùˆ ØªÙ… Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ø·Ù„Ø¨ ÙˆÙ„Ø§ ÙŠÙˆØ¬Ø¯ Ø·Ø±ÙŠÙ‚Ø© ØªÙˆØ§ØµÙ„ ÙˆÙ„Ù… ÙŠØªÙ… Ø¥Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø¨Ù„Øº.\n",
      "ÙŠØ¹Ù†ÙŠ ÙŠØ§Ø®Ø° Ø·Ù„Ø¨Ùƒ ÙˆÙŠØ±ÙˆØ­ Ù…Ø·Ø¹Ù… Ø«Ø§Ù†ÙŠØŒ ÙŠÙ†ØªØ¸Ø± Ø§Ù„Ø·Ù„Ø¨ Ø¨Ø¹Ø¯ÙŠÙ† ÙŠØ¬ÙŠÙƒ Ø§Ù„Ø§ÙƒÙ„ Ø¨Ø§Ø±Ø¯.\n",
      "Ø§Ø³ÙˆØ¡ ØªØ·Ø¨ÙŠÙ‚ ØªÙˆØµÙŠÙ„ Ù…Ù…ÙƒÙ† ØªØ´ÙˆÙÙ‡ Ø§Ù„Ø§ÙƒÙ„ ÙŠÙˆØµÙ„ Ø¨Ø§Ø±Ø¯ Ùˆ Ù†Øµ Ø§Ù„Ø§ÙƒÙ„ Ù…Ø§ÙƒÙˆÙ„ Ø§Ù„Ù„Ù‡ Ù„Ø§ ÙŠØ³Ø§Ù…Ø­Ù‡Ù… Ùˆ ØªÙˆØµÙŠÙ„Ù‡Ù… Ø§ØºÙ„Ù‰ Ù…Ù† Ø§Ù„Ø·Ù„Ø¨ Ù†ÙØ³Ù‡.\n",
      "Once the order delivered, this app will be deleted for good..\n",
      "Ø·Ù„Ø¨Øª Ø§ÙƒØ«Ø± Ù…Ù† Ù…Ø±Ù‡ Ù…Ù† Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙˆØ§Ù„Ù…Ø´ÙƒÙ„Ù‡ Ø§Ù„Ø·Ù„Ø¨ ÙŠØ§ØµÙ„ Ù…ØªØ£Ø®Ø± ÙˆØ¨Ø§Ø±Ø¯ ÙˆÙ‚Ù„ÙŠÙ„ÙŠÙ† ÙÙ‡Ù…Ù… \n",
      "ØªØ·Ø¨ÙŠÙ‚ Ù…Ù† Ø³ÙŠØ¡ Ø§Ù„Ù‰ Ø§Ø³ÙˆØ§Ø¡ â€¦ Ø§Ù„Ø³Ø¹Ø± Ø§ØºÙ„Ù‰ â€¦ Ø¯Ø§ÙŠÙ… ÙŠØªØ§Ø®Ø± ÙˆØ¯Ø§ÙŠÙ… Ø§Ù„Ø§ÙƒÙ„ ÙŠØ¬ÙŠ Ø¨Ø§Ø±Ø¯.\n",
      "Itâ€™s a really good app but it does lag from time to time and also the delivery fees are kinda expensive 15 SAR so my order can be delivered if the delivery can be a little cheaper then I would 5 stars but for now I will give it a 4.\n",
      "Ø§Ù„Ù…Ù†Ø¯ÙˆØ¨ Ù‚Ø§Ù„ Ø§Ù†ÙŠ ÙˆØµÙ„Øª Ø§Ù„Ø·Ù„Ø¨ ÙˆÙ„Ù… ÙŠØªÙ… ØªÙˆØµÙŠÙ„Ù‡ \n",
      "After waiting for 45 min the order was cancelled by the app.\n",
      "Ø­Ù‚ÙŠÙ‚Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¬Ø¯Ø§ Ø³ÙŠØ¦ ÙˆÙŠØ§Ø®Ø° Ù…Ø¨Ø§Ù„Øº Ø·Ø§Ø¦Ù„Ø© Ù„Ù„ØªÙˆØµÙŠÙ„ ÙˆØºÙŠØ± Ø¯Ù‚ÙŠÙ‚ ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ø¨Ø¯Ø§ ØŒ ÙˆÙŠÙˆØ¬Ø¯ ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§ÙØ¶Ù„ Ù…Ù†Ù‡ Ù…Ø«Ù„ Ø¬Ø§Ù‡Ø² ÙˆÙ‡Ø°Ø§ Ø§Ø®Ø± Ø·Ù„Ø¨Ø© Ø§Ø·Ù„Ø¨Ù‡ Ù…Ù† Ø¹Ù†Ø¯Ù‡Ù… Ø¨Ø¹Ø¯Ù‡Ø§ Ø¨Ø­Ø°ÙÙ‡.\n",
      "). Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ø³Ø¹Ø§Ø± Ø§Ù„ÙˆØ¬Ø¨Ø§Øª Ø¹Ù†Ø¯Ù‡ Ø§ØºÙ„Ù‰ Ù…Ù† Ø§Ù„Ù…Ø·Ø¹Ù… Ù†ÙØ³Ù‡ ÙŠØ¹Ù†ÙŠ ÙŠØ§Ø®Ø°ÙˆÙ† Ø¹Ù„ÙŠÙƒ Ù…Ø¨Ù„Øº Ø§Ù„ØªÙˆØµÙŠÙ„ ÙˆØ²ÙŠØ§Ø¯Ø© Ø¹Ù„ÙŠÙ‡ Ø±Ø§ÙØ¹ÙŠÙ† Ø³Ø¹Ø± Ø§Ù„ÙˆØ¬Ø¨Ø© Ø¹Ù† Ø³Ø¹Ø±Ù‡Ø§ Ø§Ù„Ø±Ø³Ù…ÙŠ ÙÙŠ Ø§ÙŠ Ù…Ø·Ø¹Ù… Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ù†ØµÙŠØ­Ø© ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„ØªÙˆØµÙŠÙ„ ÙƒØ«ÙŠØ±Ù‡ ÙÙŠÙ‡ Ø§ÙØ¶Ù„ Ù…Ù†Ù‡ Ø¨ Ù¡Ù Ù  Ù…Ø±Ù‡.\n"
     ]
    }
   ],
   "source": [
    "# Summarization by ratio\n",
    "\n",
    "summary_by_ratio=summarize(sentences,ratio=0.1)\n",
    "print(summary_by_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø§Ø³ÙˆØ¡ ØªØ·Ø¨ÙŠÙ‚ ØªÙˆØµÙŠÙ„ Ù…Ù…ÙƒÙ† ØªØ´ÙˆÙÙ‡ Ø§Ù„Ø§ÙƒÙ„ ÙŠÙˆØµÙ„ Ø¨Ø§Ø±Ø¯ Ùˆ Ù†Øµ Ø§Ù„Ø§ÙƒÙ„ Ù…Ø§ÙƒÙˆÙ„ Ø§Ù„Ù„Ù‡ Ù„Ø§ ÙŠØ³Ø§Ù…Ø­Ù‡Ù… Ùˆ ØªÙˆØµÙŠÙ„Ù‡Ù… Ø§ØºÙ„Ù‰ Ù…Ù† Ø§Ù„Ø·Ù„Ø¨ Ù†ÙØ³Ù‡.\n",
      "ØªØ·Ø¨ÙŠÙ‚ Ù…Ù† Ø³ÙŠØ¡ Ø§Ù„Ù‰ Ø§Ø³ÙˆØ§Ø¡ â€¦ Ø§Ù„Ø³Ø¹Ø± Ø§ØºÙ„Ù‰ â€¦ Ø¯Ø§ÙŠÙ… ÙŠØªØ§Ø®Ø± ÙˆØ¯Ø§ÙŠÙ… Ø§Ù„Ø§ÙƒÙ„ ÙŠØ¬ÙŠ Ø¨Ø§Ø±Ø¯.\n"
     ]
    }
   ],
   "source": [
    "# Summarization by word count\n",
    "\n",
    "summary_by_word_count=summarize(sentences,word_count=30)\n",
    "print(summary_by_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø§Ø³ÙˆØ¡ ØªØ·Ø¨ÙŠÙ‚ ØªÙˆØµÙŠÙ„ Ù…Ù…ÙƒÙ† ØªØ´ÙˆÙÙ‡ Ø§Ù„Ø§ÙƒÙ„ ÙŠÙˆØµÙ„ Ø¨Ø§Ø±Ø¯ Ùˆ Ù†Øµ Ø§Ù„Ø§ÙƒÙ„ Ù…Ø§ÙƒÙˆÙ„ Ø§Ù„Ù„Ù‡ Ù„Ø§ ÙŠØ³Ø§Ù…Ø­Ù‡Ù… Ùˆ ØªÙˆØµÙŠÙ„Ù‡Ù… Ø§ØºÙ„Ù‰ Ù…Ù† Ø§Ù„Ø·Ù„Ø¨ Ù†ÙØ³Ù‡.\n",
      "ØªØ·Ø¨ÙŠÙ‚ Ù…Ù† Ø³ÙŠØ¡ Ø§Ù„Ù‰ Ø§Ø³ÙˆØ§Ø¡ â€¦ Ø§Ù„Ø³Ø¹Ø± Ø§ØºÙ„Ù‰ â€¦ Ø¯Ø§ÙŠÙ… ÙŠØªØ§Ø®Ø± ÙˆØ¯Ø§ÙŠÙ… Ø§Ù„Ø§ÙƒÙ„ ÙŠØ¬ÙŠ Ø¨Ø§Ø±Ø¯.\n"
     ]
    }
   ],
   "source": [
    "# Summarization when both ratio & word count is given\n",
    "\n",
    "summary=summarize(sentences, ratio=0.1, word_count=30)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References <br>\n",
    "[1] https://towardsdatascience.com/arabic-nlp-unique-challenges-and-their-solutions-d99e8a87893d    \n",
    "[2] https://www.semrush.com/blog/pagerank/?kw=&cmp=WW_SRCH_DSA_Blog_Core_BU_EN&label=dsa_pagefeed&Network=g&Device=c&utm_content=515715493860&kwid=dsa-1057183199035&cmpid=11776868584&agpid=117384911274&BU=Core&extid=203745206953&adpos=&gclid=CjwKCAiAs92MBhAXEiwAXTi25zTaakLMojcAl13lNgOQLpRPWsdksUPyunVtrDGoS29OWVrW0eHq1hoCEIUQAvD_BwE#header2\n",
    "[3] https://cran.r-project.org/web/packages/textrank/vignettes/textrank.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "TestRank_Text_Summarization.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
